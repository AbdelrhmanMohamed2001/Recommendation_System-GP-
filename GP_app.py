from fuzzywuzzy import fuzz
import torch
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForMaskedLM
from nltk.tokenize import word_tokenize
import os
import nltk
import re
import joblib
import streamlit as st
import pandas as pd
import numpy as np
import torch
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForMaskedLM
from nltk.tokenize import word_tokenize
import pyarabic.araby as araby
from nltk.tokenize import word_tokenize
import joblib
import nltk
import streamlit as st
import tweepy
import pandas as pd
import numpy as np
import torch
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForMaskedLM
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')
import re
from nltk.stem import ARLSTem
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import string
! pip install stop-words
from stop_words import get_stop_words
from transformers import AutoTokenizer, AutoModelForMaskedLM


model_id = "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"

bert_tokenizer = AutoTokenizer.from_pretrained(model_id)
bert_model = AutoModelForMaskedLM.from_pretrained(model_id)


from simpletransformers.classification import ClassificationModel, ClassificationArgs
import pandas as pd
import logging

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
def f1_multiclass(labels, preds):
    return f1_score(labels, preds, average='macro') , accuracy_score(labels,preds)

dataset=pd.read_excel('/content/original_dataset.xlsx')

dataset=dataset[['review_description','rating']]
dataset.head()
dataset.rename(columns={'review_description':'text','rating': 'labels'}, inplace=True, errors='raise')

emojis = {
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "ğŸ˜‚":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ’”":"Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ†",
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "â¤ï¸":"Ø­Ø¨",
    "â¤":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø¨",
    "ğŸ˜­":"ÙŠØ¨ÙƒÙŠ",
    "ğŸ˜¢":"Ø­Ø²Ù†",
    "ğŸ˜”":"Ø­Ø²Ù†",
    "â™¥":"Ø­Ø¨",
    "ğŸ’œ":"Ø­Ø¨",
    "ğŸ˜…":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ™":"Ø­Ø²ÙŠÙ†",
    "ğŸ’•":"Ø­Ø¨",
    "ğŸ’™":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜Š":"Ø³Ø¹Ø§Ø¯Ø©",
    "ğŸ‘":"ÙŠØµÙÙ‚",
    "ğŸ‘Œ":"Ø§Ø­Ø³Ù†Øª",
    "ğŸ˜´":"ÙŠÙ†Ø§Ù…",
    "ğŸ˜€":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜Œ":"Ø­Ø²ÙŠÙ†",
    "ğŸŒ¹":"ÙˆØ±Ø¯Ø©",
    "ğŸ™ˆ":"Ø­Ø¨",
    "ğŸ˜„":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜":"Ù…Ø­Ø§ÙŠØ¯",
    "âœŒ":"Ù…Ù†ØªØµØ±",
    "âœ¨":"Ù†Ø¬Ù…Ù‡",
    "ğŸ¤”":"ØªÙÙƒÙŠØ±",
    "ğŸ˜":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ˜’":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ™„":"Ù…Ù„Ù„",
    "ğŸ˜•":"Ø¹ØµØ¨ÙŠØ©",
    "ğŸ˜ƒ":"ÙŠØ¶Ø­Ùƒ",
    "ğŸŒ¸":"ÙˆØ±Ø¯Ø©",
    "ğŸ˜“":"Ø­Ø²Ù†",
    "ğŸ’":"Ø­Ø¨",
    "ğŸ’—":"Ø­Ø¨",
    "ğŸ˜‘":"Ù…Ù†Ø²Ø¹Ø¬",
    "ğŸ’­":"ØªÙÙƒÙŠØ±",
    "ğŸ˜":"Ø«Ù‚Ø©",
    "ğŸ’›":"Ø­Ø¨",
    "ğŸ˜©":"Ø­Ø²ÙŠÙ†",
    "ğŸ’ª":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ‘":"Ù…ÙˆØ§ÙÙ‚",
    "ğŸ™ğŸ»":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ˜³":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ‘ğŸ¼":"ØªØµÙÙŠÙ‚",
    "ğŸ¶":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "ğŸŒš":"ØµÙ…Øª",
    "ğŸ’š":"Ø­Ø¨",
    "ğŸ™":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ’˜":"Ø­Ø¨",
    "ğŸƒ":"Ø³Ù„Ø§Ù…",
    "â˜º":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ¸":"Ø¶ÙØ¯Ø¹",
    "ğŸ˜¶":"Ù…ØµØ¯ÙˆÙ…",
    "âœŒï¸":"Ù…Ø±Ø­",
    "âœ‹ğŸ»":"ØªÙˆÙ‚Ù",
    "ğŸ˜‰":"ØºÙ…Ø²Ø©",
    "ğŸŒ·":"Ø­Ø¨",
    "ğŸ™ƒ":"Ù…Ø¨ØªØ³Ù…",
    "ğŸ˜«":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜¨":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ¼ ":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "ğŸ":"Ù…Ø±Ø­",
    "ğŸ‚":"Ù…Ø±Ø­",
    "ğŸ’Ÿ":"Ø­Ø¨",
    "ğŸ˜ª":"Ø­Ø²Ù†",
    "ğŸ˜†":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜£":"Ø§Ø³ØªÙŠØ§Ø¡",
    "â˜ºï¸":"Ø­Ø¨",
    "ğŸ˜±":"ÙƒØ§Ø±Ø«Ø©",
    "ğŸ˜":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜–":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸƒğŸ¼":"ÙŠØ¬Ø±ÙŠ",
    "ğŸ˜¡":"ØºØ¶Ø¨",
    "ğŸš¶":"ÙŠØ³ÙŠØ±",
    "ğŸ¤•":"Ù…Ø±Ø¶",
    "â€¼ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ•Š":"Ø·Ø§Ø¦Ø±",
    "ğŸ‘ŒğŸ»":"Ø§Ø­Ø³Ù†Øª",
    "â£":"Ø­Ø¨",
    "ğŸ™Š":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ’ƒ":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ’ƒğŸ¼":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ˜œ":"Ù…Ø±Ø­",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜Ÿ":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸ’–":"Ø­Ø¨",
    "ğŸ˜¥":"Ø­Ø²Ù†",
    "ğŸ»":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "âœ’":"ÙŠÙƒØªØ¨",
    "ğŸš¶ğŸ»":"ÙŠØ³ÙŠØ±",
    "ğŸ’":"Ø§Ù„Ù…Ø§Ø¸",
    "ğŸ˜·":"ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶",
    "â˜":"ÙˆØ§Ø­Ø¯",
    "ğŸš¬":"ØªØ¯Ø®ÙŠÙ†",
    "ğŸ’" : "ÙˆØ±Ø¯",
    "ğŸŒ" : "Ø´Ù…Ø³",
    "ğŸ‘†" : "Ø§Ù„Ø§ÙˆÙ„",
    "âš ï¸" :"ØªØ­Ø°ÙŠØ±",
    "ğŸ¤—" : "Ø§Ø­ØªÙˆØ§Ø¡",
    "âœ–ï¸": "ØºÙ„Ø·",
    "ğŸ“"  : "Ù…ÙƒØ§Ù†",
    "ğŸ‘¸" : "Ù…Ù„ÙƒÙ‡",
    "ğŸ‘‘" : "ØªØ§Ø¬",
    "âœ”ï¸" : "ØµØ­",
    "ğŸ’Œ": "Ù‚Ù„Ø¨",
    "ğŸ˜²" : "Ù…Ù†Ø¯Ù‡Ø´",
    "ğŸ’¦": "Ù…Ø§Ø¡",
    "ğŸš«" : "Ø®Ø·Ø§",
    "ğŸ‘ğŸ»" : "Ø¨Ø±Ø§ÙÙˆ",
    "ğŸŠ" :"ÙŠØ³Ø¨Ø­",
    "ğŸ‘ğŸ»": "ØªÙ…Ø§Ù…",
    "â­•ï¸" :"Ø¯Ø§Ø¦Ø±Ù‡ ÙƒØ¨ÙŠØ±Ù‡",
    "ğŸ·" : "Ø³Ø§ÙƒØ³ÙÙˆÙ†",
    "ğŸ‘‹": "ØªÙ„ÙˆÙŠØ­ Ø¨Ø§Ù„ÙŠØ¯",
    "âœŒğŸ¼": "Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±",
    "ğŸŒ":"Ù…Ø¨ØªØ³Ù…",
    "â¿"  : "Ø¹Ù‚Ø¯Ù‡ Ù…Ø²Ø¯ÙˆØ¬Ù‡",
    "ğŸ’ªğŸ¼" : "Ù‚ÙˆÙŠ",
    "ğŸ“©":  "ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ",
    "â˜•ï¸": "Ù‚Ù‡ÙˆÙ‡",
    "ğŸ˜§" : "Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø©",
    "ğŸ—¨": "Ø±Ø³Ø§Ù„Ø©",
    "â—ï¸" :"ØªØ¹Ø¬Ø¨",
    "ğŸ™†ğŸ»": "Ø§Ø´Ø§Ø±Ù‡ Ù…ÙˆØ§ÙÙ‚Ù‡",
    "ğŸ‘¯" :"Ø§Ø®ÙˆØ§Øª",
    "Â©" :  "Ø±Ù…Ø²",
    "ğŸ‘µğŸ½" :"Ø³ÙŠØ¯Ù‡ Ø¹Ø¬ÙˆØ²Ù‡",
    "ğŸ£": "ÙƒØªÙƒÙˆØª",
    "ğŸ™Œ": "ØªØ´Ø¬ÙŠØ¹",
    "ğŸ™‡": "Ø´Ø®Øµ ÙŠÙ†Ø­Ù†ÙŠ",
    "ğŸ‘ğŸ½":"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡",
    "ğŸ‘ŒğŸ½": "Ø¨Ø§Ù„Ø¸Ø¨Ø·",
    "â‰ï¸" : "Ø§Ø³ØªÙ†ÙƒØ§Ø±",
    "âš½ï¸": "ÙƒÙˆØ±Ù‡",
    "ğŸ•¶" :"Ø­Ø¨",
    "ğŸˆ" :"Ø¨Ø§Ù„ÙˆÙ†",
    "ğŸ€":    "ÙˆØ±Ø¯Ù‡",
    "ğŸ’µ":  "ÙÙ„ÙˆØ³",
    "ğŸ˜‹":  "Ø¬Ø§Ø¦Ø¹",
    "ğŸ˜›":  "ÙŠØºÙŠØ¸",
    "ğŸ˜ ":  "ØºØ§Ø¶Ø¨",
    "âœğŸ»":  "ÙŠÙƒØªØ¨",
    "ğŸŒ¾":  "Ø§Ø±Ø²",
    "ğŸ‘£":  "Ø§Ø«Ø± Ù‚Ø¯Ù…ÙŠÙ†",
    "âŒ":"Ø±ÙØ¶",
    "ğŸŸ":"Ø·Ø¹Ø§Ù…",
    "ğŸ‘¬":"ØµØ¯Ø§Ù‚Ø©",
    "ğŸ°":"Ø§Ø±Ù†Ø¨",
    "â˜‚":"Ù…Ø·Ø±",
    "âšœ":"Ù…Ù…Ù„ÙƒØ© ÙØ±Ù†Ø³Ø§",
    "ğŸ‘":"Ø®Ø±ÙˆÙ",
    "ğŸ—£":"ØµÙˆØª Ù…Ø±ØªÙØ¹",
    "ğŸ‘ŒğŸ¼":"Ø§Ø­Ø³Ù†Øª",
    "â˜˜":"Ù…Ø±Ø­",
    "ğŸ˜®":"ØµØ¯Ù…Ø©",
    "ğŸ˜¦":"Ù‚Ù„Ù‚",
    "â­•":"Ø§Ù„Ø­Ù‚",
    "âœï¸":"Ù‚Ù„Ù…",
    "â„¹":"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª",
    "ğŸ™ğŸ»":"Ø±ÙØ¶",
    "âšªï¸":"Ù†Ø¶Ø§Ø±Ø© Ù†Ù‚Ø§Ø¡",
    "ğŸ¤":"Ø­Ø²Ù†",
    "ğŸ’«":"Ù…Ø±Ø­",
    "ğŸ’":"Ø­Ø¨",
    "ğŸ”":"Ø·Ø¹Ø§Ù…",
    "â¤ï¸":"Ø­Ø¨",
    "âœˆï¸":"Ø³ÙØ±",
    "ğŸƒğŸ»â€â™€ï¸":"ÙŠØ³ÙŠØ±",
    "ğŸ³":"Ø°ÙƒØ±",
    "ğŸ¤":"Ù…Ø§ÙŠÙƒ ØºÙ†Ø§Ø¡",
    "ğŸ¾":"ÙƒØ±Ù‡",
    "ğŸ”":"Ø¯Ø¬Ø§Ø¬Ø©",
    "ğŸ™‹":"Ø³Ø¤Ø§Ù„",
    "ğŸ“®":"Ø¨Ø­Ø±",
    "ğŸ’‰":"Ø¯ÙˆØ§Ø¡",
    "ğŸ™ğŸ¼":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ’‚ğŸ¿ ":"Ø­Ø§Ø±Ø³",
    "ğŸ¬":"Ø³ÙŠÙ†Ù…Ø§",
    "â™¦ï¸":"Ù…Ø±Ø­",
    "ğŸ’¡":"Ù‚ÙƒØ±Ø©",
    "â€¼":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘¼":"Ø·ÙÙ„",
    "ğŸ”‘":"Ù…ÙØªØ§Ø­",
    "â™¥ï¸":"Ø­Ø¨",
    "ğŸ•‹":"ÙƒØ¹Ø¨Ø©",
    "ğŸ“":"Ø¯Ø¬Ø§Ø¬Ø©",
    "ğŸ’©":"Ù…Ø¹ØªØ±Ø¶",
    "ğŸ‘½":"ÙØ¶Ø§Ø¦ÙŠ",
    "â˜”ï¸":"Ù…Ø·Ø±",
    "ğŸ·":"Ø¹ØµÙŠØ±",
    "ğŸŒŸ":"Ù†Ø¬Ù…Ø©",
    "â˜ï¸":"Ø³Ø­Ø¨",
    "ğŸ‘ƒ":"Ù…Ø¹ØªØ±Ø¶",
    "ğŸŒº":"Ù…Ø±Ø­",
    "ğŸ”ª":"Ø³ÙƒÙŠÙ†Ø©",
    "â™¨":"Ø³Ø®ÙˆÙ†ÙŠØ©",
    "ğŸ‘ŠğŸ¼":"Ø¶Ø±Ø¨",
    "âœ":"Ù‚Ù„Ù…",
    "ğŸš¶ğŸ¾â€â™€ï¸":"ÙŠØ³ÙŠØ±",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "â—¾ï¸":"ÙˆÙ‚Ù",
    "ğŸ˜š":"Ø­Ø¨",
    "ğŸ”¸":"Ù…Ø±Ø­",
    "ğŸ‘ğŸ»":"Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ",
    "ğŸ‘ŠğŸ½":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜™":"Ø­Ø¨",
    "ğŸ¥":"ØªØµÙˆÙŠØ±",
    "ğŸ‘‰":"Ø¬Ø°Ø¨ Ø§Ù†ØªØ¨Ø§Ù‡",
    "ğŸ‘ğŸ½":"ÙŠØµÙÙ‚",
    "ğŸ’ªğŸ»":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ´":"Ø§Ø³ÙˆØ¯",
    "ğŸ”¥":"Ø­Ø±ÙŠÙ‚",
    "ğŸ˜¬":"Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©",
    "ğŸ‘ŠğŸ¿":"ÙŠØ¶Ø±Ø¨",
    "ğŸŒ¿":"ÙˆØ±Ù‚Ù‡ Ø´Ø¬Ø±Ù‡",
    "âœ‹ğŸ¼":"ÙƒÙ Ø§ÙŠØ¯",
    "ğŸ‘":"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡",
    "â˜ ï¸":"ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨",
    "ğŸ‰":"ÙŠÙ‡Ù†Ø¦",
    "ğŸ”•" :"ØµØ§Ù…Øª",
    "ğŸ˜¿":"ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ†",
    "â˜¹ï¸":"ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³",
    "ğŸ˜˜" :"Ø­Ø¨",
    "ğŸ˜°" :"Ø®ÙˆÙ Ùˆ Ø­Ø²Ù†",
    "ğŸŒ¼":"ÙˆØ±Ø¯Ù‡",
    "ğŸ’‹":  "Ø¨ÙˆØ³Ù‡",
    "ğŸ‘‡":"Ù„Ø§Ø³ÙÙ„",
    "â£ï¸":"Ø­Ø¨",
    "ğŸ§":"Ø³Ù…Ø§Ø¹Ø§Øª",
    "ğŸ“":"ÙŠÙƒØªØ¨",
    "ğŸ˜‡":"Ø¯Ø§ÙŠØ®",
    "ğŸ˜ˆ":"Ø±Ø¹Ø¨",
    "ğŸƒ":"ÙŠØ¬Ø±ÙŠ",
    "âœŒğŸ»":"Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±",
    "ğŸ”«":"ÙŠØ¶Ø±Ø¨",
    "â—ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘":"ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚",
    "ğŸ”":"Ù‚ÙÙ„",
    "ğŸ‘ˆ":"Ù„Ù„ÙŠÙ…ÙŠÙ†",
    "â„¢":"Ø±Ù…Ø²",
    "ğŸš¶ğŸ½":"ÙŠØªÙ…Ø´ÙŠ",
    "ğŸ˜¯":"Ù…ØªÙØ§Ø¬Ø£",
    "âœŠ":"ÙŠØ¯ Ù…ØºÙ„Ù‚Ù‡",
    "ğŸ˜»":"Ø§Ø¹Ø¬Ø§Ø¨",
    "ğŸ™‰" :"Ù‚Ø±Ø¯",
    "ğŸ‘§":"Ø·ÙÙ„Ù‡ ØµØºÙŠØ±Ù‡",
    "ğŸ”´":"Ø¯Ø§Ø¦Ø±Ù‡ Ø­Ù…Ø±Ø§Ø¡",
    "ğŸ’ªğŸ½":"Ù‚ÙˆÙ‡",
    "ğŸ’¤":"ÙŠÙ†Ø§Ù…",
    "ğŸ‘€":"ÙŠÙ†Ø¸Ø±",
    "âœğŸ»":"ÙŠÙƒØªØ¨",
    "â„ï¸":"ØªÙ„Ø¬",
    "ğŸ’€":"Ø±Ø¹Ø¨",
    "ğŸ˜¤":"ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³",
    "ğŸ–‹":"Ù‚Ù„Ù…",
    "ğŸ©":"ÙƒØ§Ø¨",
    "â˜•ï¸":"Ù‚Ù‡ÙˆÙ‡",
    "ğŸ˜¹":"Ø¶Ø­Ùƒ",
    "ğŸ’“":"Ø­Ø¨",
    "â˜„ï¸ ":"Ù†Ø§Ø±",
    "ğŸ‘»":"Ø±Ø¹Ø¨",
    "â":"Ø®Ø·Ø¡",
    "ğŸ¤®":"Ø­Ø²Ù†",
    'ğŸ»':"Ø§Ø­Ù…Ø±"
    }

nltk.download('punkt')
arr = os.listdir('test_data')
all_datasets=[]
names=[]

import streamlit as st
st.session_state['max_value']=-10000
rec=" "



##################### hendle_emojis ###############################
def hendle_emojis(text,emojis):
  li=[]
  for i in word_tokenize(text):
    for x in i :
      if x not in emojis.keys():
        li.append(i)
        break
      else:
        li.append(emojis[x])
  return " ".join(li)
####################### data cleaning ##########################
def data_cleaning(x):
    new = re.sub(r'[^Ø¡-ÙŠ]',' ',x)
    return new
################remove_diacritics##############################
def remove_diacritics(text):
    arabic_diacritics = re.compile(""" Ù‘    | # Tashdid
                             Ù    | # Fatha
                             Ù‹    | # Tanwin Fath
                             Ù    | # Damma
                             ÙŒ    | # Tanwin Damm
                             Ù    | # Kasra
                             Ù    | # Tanwin Kasr
                             Ù’    | # Sukun
                             Ù€     # Tatwil/Kashida
                         """, re.VERBOSE)
    text = re.sub(arabic_diacritics, '', str(text))
    return text
################################################################################    
dataset['text'] = dataset['text'].apply(handle_emojis, args=[emojis])
dataset['text']=dataset['text'].apply(data_cleaning)
dataset['text']=dataset['text'].apply(remove_diacritics)
dataset['text']=dataset['text'].apply(remove_duplicates)
################################################################################
label_counts = dataset["labels"].value_counts()
# print the label counts
print(label_counts)
################   decoding outputs   ##############################

def decode(x):
  if x == 0:
    return "Negative"
  elif x == 1:
    return "Neutral"
  else:
    return "Positive"
##############################################################

label_counts = dataset["labels"].value_counts()

# print the label counts
print(label_counts)
##############################################################

dataset['labels']=dataset['labels'].apply(encode)
num_rows=dataset.shape[0]
dataset = dataset.sample(frac = 1)
     

train=dataset[:int(num_rows*1)]
test=dataset[int(num_rows*.97)+1:]

     

model_id = "CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment"

# Optional model configuration
model_args = ClassificationArgs(num_train_epochs=2, overwrite_output_dir=True)

# Create a ClassificationModel
model = ClassificationModel(
    'bert',
    model_id,
    num_labels=3,
    args=model_args,
    use_cuda=True
)
model.train_model(train)
result, model_outputs, wrong_predictions = model.eval_model(test, f1_multiclass=f1_multiclass)

print(result)
result_train, model_outputs_train, wrong_predictions_train = model.eval_model(train, f1_multiclass=f1_multiclass)

print(result_train)

pred=[]
pred_train=[]
for i in model_outputs:
  pred.append(np.argmax(i))
for i in model_outputs_train:
  pred_train.append(np.argmax(i))
     

from sklearn.metrics import confusion_matrix,mean_squared_error,precision_score,recall_score,f1_score ,classification_report
from sklearn import metrics
     

print("Test accuracy  :",metrics.accuracy_score(pred, test['labels']) *100 ,"%")
print("MSE [test]         :",mean_squared_error(test['labels'],pred ))
     

import matplotlib.pyplot as plt
import seaborn as sns
     

cf1 = confusion_matrix(test['labels'],pred)
sns.heatmap(cf1,annot=True,fmt = '.0f')
plt.xlabel('prediction')
plt.ylabel('Actual')
plt.title(' Confusion Matrix')
plt.show()
print("           classification_report for test")
print(classification_report(test['labels'], pred))
print("           classification_report for train")
print(classification_report(train['labels'],pred_train))
print("Train accuracy  :",metrics.accuracy_score(pred_train, train['labels']) *100 ,"%")
     

dataset.head(20)
     

# Evaluate the model
result, model_outputs, wrong_predictions = model.eval_model(dataset[50000:67000], f1_multiclass=f1_multiclass)


     

wrong_predictions[:50]
     

import joblib

# train a scikit-learn model
model = model

# save the model to a file
joblib.dump(model, 'new_model(1).pkl')
#load model
model=joblib.load('new_model(1).pkl')
#################### Title ######################
st.title("WELCOME TO OUR RECOMMENDATION SYSTEM ğŸ˜ğŸ˜ğŸ˜")
st.write("WE ARE HERE TO HELP YOU...........!!!")




counter=0
num= st.number_input("Enter The number of inputs ")
for i in range (int(num)):

    file_name = st.text_input(" Input Number " +str(counter+1),"")
    counter+=1
    if file_name=="":
        break

    if file_name+".csv" not in arr:
      max_index =0
      max=fuzz.ratio(file_name,arr[0])
    for i in range(len(arr)):
      x=fuzz.ratio(file_name+".csv",arr[i])
      if x > max:
        max_index=i
        max=x
    file_name=arr[max_index]
    names.append(file_name)

for z in names:
    dataset = pd.read_csv(f'test_data/{z}')
    dataset.columns.values[0] = "text"
    dataset['label'] =0
    #st.dataframe(dataset)
    dataset=dataset.dropna()
    dataset=dataset.drop_duplicates()
    dataset['text'] = dataset['text'].apply(hendle_emojis, args=[emojis])
    dataset['text'] = dataset['text'].apply(lambda x: data_cleaning(x))
    dataset['text'] = dataset['text'].apply(lambda x: remove_diacritics(x))
    dataset = dataset.dropna()
    dataset = dataset.drop_duplicates()
    result, model_outputs, wrong_predictions = model.eval_model(dataset)

    # Get the index of the maximum value for each row

    max_index_per_row = np.argmax(model_outputs, axis=1)
    decoded_labels = [decode(x) for x in max_index_per_row ]


# Print the decoded labels
    data = [[text, label] for text, label in zip(dataset['text'], decoded_labels)]
    new_df = pd.DataFrame(data, columns=['text', 'label'])
    all_datasets.append(new_df)

    #Count the number of positive, negative, and neutral labels
    pos_count = decoded_labels.count('Positive')
    neg_count = decoded_labels.count('Negative')
    neu_count = decoded_labels.count('Neutral')

    #Visualize the counts using Streamlit and Plotly
    import streamlit as st
    import plotly.graph_objects as go

    st.write("Info about:", re.sub(r'[^Ø¡-ÙŠ ]', ' ', z))
    st.write('Number of Positive Labels:', pos_count)
    st.write('Number of Negative Labels:', neg_count)
    st.write('Number of Neutral Labels:', neu_count)

    labels = ['Positive', 'Negative', 'Neutral']
    counts = [pos_count, neg_count, neu_count]

    fig = go.Figure(data=[go.Pie(labels=labels, values=counts)])

# Adjust the height and width of the plot
    fig.update_layout(
    height=500,  # Adjust the height to your desired size
    width=500,  # Adjust the width to your desired size
    )

    st.plotly_chart(fig)

    if (pos_count /neg_count) > st.session_state['max_value']:
       rec = re.sub(r'[^Ø¡-ÙŠ ]', ' ', z)
       st.session_state['max_value'] = (pos_count / neg_count)



st.write("HERE'S THE BEST THING FOR YOU ğŸ˜ğŸ‘‰", f"{rec} \U0001F48E")


